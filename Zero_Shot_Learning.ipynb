{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Zero-Shot Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cpr4rQGU81oj",
        "2QohOXKz7-qx",
        "iLGGmnTn9TDV",
        "9waZL2IvAKC2",
        "Gh6eNCO6DXkW",
        "uXL3jQfMEZWr",
        "s3sYKzOXFxMq",
        "hl6S61e-HwF5",
        "6CiEkSW7H-ka",
        "u4r5GDXyIs5G"
      ],
      "authorship_tag": "ABX9TyNEW2PZfz9SrvZqXlBZdv4s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luca-rossi/zero-shot-learning/blob/main/Zero_Shot_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XJZ63ti45pa"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpr4rQGU81oj"
      },
      "source": [
        "### Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USxn8u0jI-mb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My\\ Drive/zsl-data//         # TODO change it to your path!!!\n",
        "!unzip \"data.zip\" -d \"/content\"\n",
        "!pip install tensorflow==2.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sThwZ2f830L"
      },
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from sklearn import preprocessing\n",
        "from sklearn.neighbors import KDTree\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "\n",
        "disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QohOXKz7-qx"
      },
      "source": [
        "### Datasets settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rBtdpwB8CXL"
      },
      "source": [
        "DATASET_CUB = 'cub'\n",
        "DATASET_AWA = 'awa'\n",
        "DATASET_SUN = 'sun'\n",
        "\n",
        "PARAMS = {\n",
        "    DATASET_CUB: {\n",
        "        'file_separator': ' ',\n",
        "        'input_shape': 2048,\n",
        "        'n_classes': 200,\n",
        "        'n_unseen_classes': 50,\n",
        "        'n_attributes': 312,\n",
        "        'n_epochs': 70,\n",
        "        'n_epochs_cls': 25,\n",
        "        'batch_size': 64,\n",
        "        'batch_size_cls': 64,\n",
        "        'learning_rate': 0.0001,\n",
        "        'learning_rate_cls': 0.001,\n",
        "        'beta': 0.5,\n",
        "        'syn_num': 300,\n",
        "        'gradient_penalty_weight': 10,\n",
        "        'cls_loss_weight': 0.01,\n",
        "        'n_critic': 5,\n",
        "        'latent_dim': 312\n",
        "    },\n",
        "    DATASET_AWA: {\n",
        "        'file_separator': '\\t',\n",
        "        'input_shape': 2048,\n",
        "        'n_classes': 50,\n",
        "        'n_unseen_classes': 10,\n",
        "        'n_attributes': 85,\n",
        "        'n_epochs': 30,\n",
        "        'n_epochs_cls': 25,\n",
        "        'batch_size': 64,\n",
        "        'batch_size_cls': 64,\n",
        "        'learning_rate': 0.00001,\n",
        "        'learning_rate_cls': 0.001,\n",
        "        'beta': 0.5,\n",
        "        'syn_num': 1800,\n",
        "        'gradient_penalty_weight': 10,\n",
        "        'cls_loss_weight': 0.01,\n",
        "        'n_critic': 5,\n",
        "        'latent_dim': 85\n",
        "    },\n",
        "    DATASET_SUN: {\n",
        "        'file_separator': '\\t',\n",
        "        'input_shape': 2048,\n",
        "        'n_classes': 717,\n",
        "        'n_unseen_classes': 72,\n",
        "        'n_attributes': 102,\n",
        "        'n_epochs': 40,\n",
        "        'n_epochs_cls': 25,\n",
        "        'batch_size': 64,\n",
        "        'batch_size_cls': 64,\n",
        "        'learning_rate': 0.0002,\n",
        "        'learning_rate_cls': 0.001,\n",
        "        'beta': 0.5,\n",
        "        'syn_num': 400,\n",
        "        'gradient_penalty_weight': 10,\n",
        "        'cls_loss_weight': 0.01,\n",
        "        'n_critic': 5,\n",
        "        'latent_dim': 102\n",
        "    },\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLGGmnTn9TDV"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1A-NTJL9UaD"
      },
      "source": [
        "GAN = True\n",
        "LOAD = True\n",
        "GZSL = True\n",
        "DATASET = DATASET_AWA\n",
        "\n",
        "INPUT_SHAPE = PARAMS[DATASET]['input_shape']\n",
        "N_CLASSES = PARAMS[DATASET]['n_classes']\n",
        "N_UNSEEN_CLASSES = PARAMS[DATASET]['n_unseen_classes']\n",
        "N_ATTRIBUTES = PARAMS[DATASET]['n_attributes']\n",
        "\n",
        "LATENT_DIM = PARAMS[DATASET]['latent_dim']\n",
        "N_CRITIC = PARAMS[DATASET]['n_critic']\n",
        "N_EPOCHS_GAN = PARAMS[DATASET]['n_epochs']\n",
        "N_EPOCHS_CLS = PARAMS[DATASET]['n_epochs_cls']\n",
        "N_BATCH_GAN = PARAMS[DATASET]['batch_size']\n",
        "N_BATCH_CLS = PARAMS[DATASET]['batch_size_cls']\n",
        "LEARNING_RATE = PARAMS[DATASET]['learning_rate']\n",
        "LEARNING_RATE_CLS = PARAMS[DATASET]['learning_rate_cls']\n",
        "BETA = PARAMS[DATASET]['beta']\n",
        "\n",
        "GRADIENT_PENALTY_WEIGHT = PARAMS[DATASET]['gradient_penalty_weight']\n",
        "CLS_LOSS_WEIGHT = PARAMS[DATASET]['cls_loss_weight']\n",
        "N_SAMPLES_GAN = N_UNSEEN_CLASSES * PARAMS[DATASET]['syn_num']\n",
        "\n",
        "PATH = '/content/data'\n",
        "FEATURES_PATH = PATH + '/' + DATASET + '/res101.mat'\n",
        "ATTRIBUTES_PATH = PATH + '/' + DATASET + '/att_splits.mat'\n",
        "FILE_SEPARATOR = PARAMS[DATASET]['file_separator']\n",
        "MODEL_SAVE_PATH = 'cgan_generator.h5'\n",
        "SAVE_MODEL_EVERY = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9waZL2IvAKC2"
      },
      "source": [
        "### Data loading functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RULBgSgqArVD"
      },
      "source": [
        "Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IxP_ExWA1it"
      },
      "source": [
        "TRAINVAL_COL = 'trainval_loc'\n",
        "TRAIN_COL = 'train_loc'\n",
        "VAL_COL = 'val_loc'\n",
        "TEST_COL = 'test_unseen_loc'\n",
        "TEST_SEEN_COL = 'test_seen_loc'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lILJx7PA95Q"
      },
      "source": [
        "From the .mat files extract all the features from resnet and the attribute splits. \n",
        "- The res101 contains features and the corresponding labels.\n",
        "- att_splits contains the different splits for trainval, train, val and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnLToU3iBBaX"
      },
      "source": [
        "def get_mat():\n",
        "    res101 = scipy.io.loadmat(FEATURES_PATH)\n",
        "    att_splits = scipy.io.loadmat(ATTRIBUTES_PATH)\n",
        "    return res101, att_splits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiJ8SFUECSlL"
      },
      "source": [
        "We need the corresponding ground-truth labels/classes for each training example for all our train, val, trainval and test set according to the split locations provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8er81sFaCYRc"
      },
      "source": [
        "def map_labels(source, dest):\n",
        "    for label in source:\n",
        "        dest[dest == label] = label\n",
        "    return dest\n",
        "\n",
        "\n",
        "def get_labels(res101, att_splits):\n",
        "    labels = dict()\n",
        "    labels['all'] = res101['labels'] - 1\n",
        "\n",
        "    labels['train'] = labels['all'][np.squeeze(att_splits[TRAIN_COL] - 1)]\n",
        "    labels['val'] = labels['all'][np.squeeze(att_splits[VAL_COL] - 1)]\n",
        "    labels['trainval'] = labels['all'][np.squeeze(att_splits[TRAINVAL_COL] - 1)]\n",
        "    labels['test'] = labels['all'][np.squeeze(att_splits[TEST_COL] - 1)]\n",
        "    labels['testval'] = labels['all'][np.squeeze(att_splits[TEST_SEEN_COL] - 1)]\n",
        "\n",
        "    labels['train_seen'] = np.unique(labels['train'])\n",
        "    labels['val_unseen'] = np.unique(labels['val'])\n",
        "    labels['trainval_seen'] = np.unique(labels['trainval'])\n",
        "    labels['test_unseen'] = np.unique(labels['test'])\n",
        "    labels['test_seen'] = np.unique(labels['testval'])\n",
        "\n",
        "    labels['train'] = map_labels(labels['train_seen'], labels['train'])\n",
        "    labels['val'] = map_labels(labels['val_unseen'], labels['val'])\n",
        "    labels['trainval'] = map_labels(labels['trainval_seen'], labels['trainval'])\n",
        "    labels['test'] = map_labels(labels['test_unseen'], labels['test'])\n",
        "    labels['testval'] = map_labels(labels['test_seen'], labels['testval'])\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O2aaZGYCfle"
      },
      "source": [
        "Let us denote the features X ∈ [d×m] available at training stage,\n",
        "where d is the dimensionality of the data, and m is the number of instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uY9DhT5C-Pl"
      },
      "source": [
        "def get_features(res101, att_splits):\n",
        "    features = dict()\n",
        "    scaler = preprocessing.MinMaxScaler()\n",
        "    features['all'] = res101['features'].transpose()\n",
        "\n",
        "    features['trainval'] = features['all'][np.squeeze(att_splits[TRAINVAL_COL] - 1), :]\n",
        "    features['trainval'] = scaler.fit_transform(features['trainval'])\n",
        "    features['train'] = features['all'][np.squeeze(att_splits[TRAIN_COL] - 1), :]\n",
        "    features['train'] = scaler.transform(features['train'])\n",
        "    features['val'] = features['all'][np.squeeze(att_splits[VAL_COL] - 1), :]\n",
        "    features['val'] = scaler.transform(features['val'])\n",
        "    features['test'] = features['all'][np.squeeze(att_splits[TEST_COL] - 1), :]\n",
        "    features['test'] = scaler.transform(features['test'])\n",
        "    features['testval'] = features['all'][np.squeeze(att_splits[TEST_SEEN_COL] - 1), :]\n",
        "    features['testval'] = scaler.transform(features['testval'])\n",
        "    features['all'] = scaler.transform(features['all'])\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaCi2pgADCck"
      },
      "source": [
        "Each of the classes in the dataset have an attribute (a) description.\n",
        "This vector is known as the `Signature matrix` of dimension S ∈ [0, 1]a×z.\n",
        "For training stage there are z classes and z' classes for test S ∈ [0, 1]a×z'.\n",
        "The occurance of an attribute corresponding to the class is given.\n",
        "For instance, if the classes are `horse` and `zebra` and the corresponding attributes are\n",
        "[wild_animal, 4_legged, carnivore]\n",
        "```\n",
        " Horse      Zebra\n",
        "[0.00354613 0.        ] Domestic_animal\n",
        "[0.13829921 0.20209503] 4_legged\n",
        "[0.06560347 0.04155225] carnivore\n",
        "```\n",
        "att_splits keys: 'allclasses_names', 'att', 'original_att', 'test_seen_loc', 'test_unseen_loc', 'train_loc', 'trainval_loc', 'val_loc'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhI2s4GvAQBk"
      },
      "source": [
        "def get_signatures(att_splits):\n",
        "    attrs = att_splits['att'].transpose()\n",
        "    signatures = list()\n",
        "    for i, attr in enumerate(attrs):\n",
        "        signatures.append((i, attr))\n",
        "    classnames, signatures = zip(*signatures)\n",
        "    classnames = list(classnames)\n",
        "    signatures = np.asarray(signatures, dtype=np.float)\n",
        "    return signatures, classnames\n",
        "\n",
        "\n",
        "def get_attributes(labels, signatures):\n",
        "    attributes = dict()\n",
        "    attributes['all'] = np.array([signatures[y] for y in labels['all']]).squeeze()\n",
        "\n",
        "    attributes['train'] = np.array([signatures[y] for y in labels['train']]).squeeze()\n",
        "    attributes['val'] = np.array([signatures[y] for y in labels['val']]).squeeze()\n",
        "    attributes['trainval'] = np.array([signatures[y] for y in labels['trainval']]).squeeze()\n",
        "    attributes['test'] = np.array([signatures[y] for y in labels['test']]).squeeze()\n",
        "    attributes['testval'] = np.array([signatures[y] for y in labels['testval']]).squeeze()\n",
        "    return attributes\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    res101, att_splits = get_mat()\n",
        "    labels = get_labels(res101, att_splits)\n",
        "    features = get_features(res101, att_splits)\n",
        "    signatures, classnames = get_signatures(att_splits)\n",
        "    attributes = get_attributes(labels, signatures)\n",
        "    return features, attributes, labels, signatures, classnames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh6eNCO6DXkW"
      },
      "source": [
        "### Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY6ZTM43Dfz4"
      },
      "source": [
        "features, attributes, labels, signatures, classnames = load_data()\n",
        "train_seenX, train_seenA, train_seenY = features['trainval'], attributes['trainval'], labels['trainval']\n",
        "test_unseenX, test_unseenA, test_unseenY = features['test'], attributes['test'], labels['test']\n",
        "test_seenX, test_seenA, test_seenY = features['testval'], attributes['testval'], labels['testval']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk-XlW324tF6"
      },
      "source": [
        "# Embedded method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXL3jQfMEZWr"
      },
      "source": [
        "### Embedded classifier definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dai3eShF4NxV"
      },
      "source": [
        "CLASS_VECTORS = list()\n",
        "\n",
        "\n",
        "def custom_kernel_init(shape, dtype=None):\n",
        "    return CLASS_VECTORS.T\n",
        "\n",
        "\n",
        "class EmbeddedClassifier:\n",
        "    def __init__(self, signatures, input_shape=INPUT_SHAPE, learning_rate=LEARNING_RATE_CLS, beta=BETA):\n",
        "        self.model = self.__build_model(signatures, input_shape, learning_rate, beta)\n",
        "\n",
        "    def __build_model(self, signatures, input_shape, learning_rate, beta):\n",
        "        global CLASS_VECTORS\n",
        "        CLASS_VECTORS = signatures\n",
        "        model = Sequential()\n",
        "        model.add(Dense(signatures.shape[1], input_shape=(input_shape,), activation='relu', kernel_initializer=RandomNormal(stddev=0.02)))\n",
        "        model.add(Dense(signatures.shape[0], activation='softmax', trainable=False, kernel_initializer=custom_kernel_init))\n",
        "        print('-----------------------')\n",
        "        print('Classifier')\n",
        "        model.summary()\n",
        "        adam = Adam(lr=learning_rate, beta_1=beta)\n",
        "        model.compile(loss=SparseCategoricalCrossentropy(from_logits=True), optimizer=adam, metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def train(self, trainX, trainY, n_batch=N_BATCH_CLS, n_epochs=N_EPOCHS_CLS):\n",
        "        self.model.fit(trainX, trainY, verbose=2, epochs=n_epochs, batch_size=n_batch, shuffle=True)\n",
        "\n",
        "    def eval(self, x, y, classnames, signatures):\n",
        "        print()\n",
        "        #score = self.model.evaluate(x, y, verbose=0)\n",
        "        #print('Test loss:', score[0])\n",
        "        #print('Test accuracy:', score[1])\n",
        "        inp = self.model.input\n",
        "        out = self.model.layers[-2].output\n",
        "        model = Model(inp, out)\n",
        "        predY = model.predict(x)\n",
        "        tree = KDTree(signatures)\n",
        "        top5, top3, top1 = 0, 0, 0\n",
        "        for i, pred in enumerate(predY):\n",
        "            pred = np.expand_dims(pred, axis=0)\n",
        "            dist_5, index_5 = tree.query(pred, k=5)\n",
        "            # TODO fix index_5 !!!\n",
        "            pred_labels = [classnames[index] for index in index_5[0]]\n",
        "            true_label = y[i]\n",
        "            if true_label in pred_labels:\n",
        "                top5 += 1\n",
        "            if true_label in pred_labels[:3]:\n",
        "                top3 += 1\n",
        "            if true_label == pred_labels[0]:\n",
        "                top1 += 1\n",
        "        print(\"ZERO SHOT LEARNING SCORE\")\n",
        "        print(\"-> Top-5 Accuracy: %.2f\" % (top5 / float(len(x))))\n",
        "        print(\"-> Top-3 Accuracy: %.2f\" % (top3 / float(len(x))))\n",
        "        print(\"-> Top-1 Accuracy: %.2f\" % (top1 / float(len(x))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3sYKzOXFxMq"
      },
      "source": [
        "### Embedded method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyDKP1vKFzIU"
      },
      "source": [
        "cls = EmbeddedClassifier(signatures)\n",
        "cls.train(train_seenX, train_seenY)\n",
        "signatures_eval = signatures if GZSL else signatures[labels['test_unseen'], :]\n",
        "cls.eval(test_unseenX, test_unseenY, classnames, signatures_eval)\n",
        "cls.eval(test_seenX, test_seenY, classnames, signatures_eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VnAu14i4i5k"
      },
      "source": [
        "# Generative method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl6S61e-HwF5"
      },
      "source": [
        "### Classifier definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT3vBN774rLo"
      },
      "source": [
        "class Classifier:\n",
        "    def __init__(self, input_shape=INPUT_SHAPE, learning_rate=LEARNING_RATE_CLS, beta=BETA):\n",
        "        self.model = self.__build_model(input_shape, learning_rate, beta)\n",
        "\n",
        "    def __build_model(self, input_shape, learning_rate, beta):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(N_CLASSES, input_shape=(input_shape,), activation='softmax', kernel_initializer=RandomNormal(stddev=0.02)))\n",
        "        print('-----------------------')\n",
        "        print('Classifier')\n",
        "        model.summary()\n",
        "        feature = Input(shape=(input_shape,))\n",
        "        classes = model(feature)\n",
        "        model = Model(feature, classes)\n",
        "        opt = Adam(learning_rate=learning_rate, beta_1=beta, beta_2=0.999)\n",
        "        loss = SparseCategoricalCrossentropy(from_logits=True)\n",
        "        model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def train(self, x, y, n_batch=N_BATCH_CLS, n_epochs=N_EPOCHS_CLS):\n",
        "        self.model.fit(x, y, batch_size=n_batch, epochs=n_epochs, shuffle=True, verbose=1)\n",
        "\n",
        "    def train_epoch(self, x, y, n_batch=N_BATCH_CLS):\n",
        "        idx = np.random.permutation(len(x))\n",
        "        x, y = x[idx], y[idx]\n",
        "        n_batches = int(len(x) / n_batch)\n",
        "        for batch in range(n_batches):\n",
        "            batchX = x[batch * n_batch : (batch + 1) * n_batch]\n",
        "            batchY = y[batch * n_batch : (batch + 1) * n_batch]\n",
        "            self.model.train_on_batch(batchX, batchY)\n",
        "\n",
        "    def eval(self, x, y):\n",
        "        score = self.model.evaluate(x, y, verbose=0)\n",
        "        print('Test loss:', score[0])\n",
        "        print('Test accuracy:', score[1])\n",
        "\n",
        "    def get_per_class_accuracy(self, test_X, test_Y, target_classes):\n",
        "        test_Y = test_Y.squeeze()\n",
        "        predicted_label = self.model.predict(test_X).argmax(axis=-1)\n",
        "        acc_per_class = 0\n",
        "        for i in target_classes:\n",
        "            idx = (test_Y == i)\n",
        "            acc_per_class += np.sum(test_Y[idx] == predicted_label[idx]) / np.sum(idx)\n",
        "        acc_per_class /= target_classes.shape[0]\n",
        "        return acc_per_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CiEkSW7H-ka"
      },
      "source": [
        "### GAN definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szcz8i9ZIAm-"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, ReLU, LeakyReLU, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "from losses import wasserstein_loss, gradient_penalty_loss\n",
        "from config import *\n",
        "\n",
        "\n",
        "class RandomWeightedAverage(tf.keras.layers.Layer):\n",
        "    def call(self, inputs, **kwargs):\n",
        "        alpha = backend.random_uniform((inputs[2], 1))\n",
        "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
        "\n",
        "\n",
        "class Gan:\n",
        "    def __init__(self, classifier, n_attributes=N_ATTRIBUTES, input_shape=INPUT_SHAPE, n_batch=N_BATCH_GAN, latent_dim=LATENT_DIM):\n",
        "        self.critic = self.__build_critic(n_attributes, input_shape)\n",
        "        self.generator = self.__build_generator(n_attributes, latent_dim)\n",
        "        self.classifier = classifier\n",
        "        self.compiled_critic = self.__compile_critic(n_batch)\n",
        "        self.compiled_generator = self.__compile_generator()\n",
        "\n",
        "    def __build_critic(self, n_attributes, input_shape):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(4096, input_dim=(n_attributes + input_shape), kernel_initializer=RandomNormal(stddev=0.02)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, kernel_initializer=RandomNormal(stddev=0.02)))\n",
        "        print('-----------------------')\n",
        "        print('Critic')\n",
        "        model.summary()\n",
        "        feature = Input(shape=(input_shape,))\n",
        "        label = Input(shape=(n_attributes,))\n",
        "        model_input = Concatenate()([feature, label])\n",
        "        validity = model(model_input)\n",
        "        return Model([feature, label], validity)\n",
        "\n",
        "    def __build_generator(self, n_attributes, latent_dim):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(4096, input_dim=(n_attributes + latent_dim), kernel_initializer=RandomNormal(stddev=0.02)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(INPUT_SHAPE, kernel_initializer=RandomNormal(stddev=0.02)))\n",
        "        model.add(ReLU())\n",
        "        print('-----------------------')\n",
        "        print('Generator')\n",
        "        model.summary()\n",
        "        noise = Input(shape=(latent_dim,))\n",
        "        label = Input(shape=(n_attributes,))\n",
        "        model_input = Concatenate()([noise, label])\n",
        "        img = model(model_input)\n",
        "        return Model([noise, label], img)\n",
        "\n",
        "    def __compile_critic(self, n_batch):\n",
        "        # Freeze generator's layers while training critic\n",
        "        self.critic.trainable = True\n",
        "        self.generator.trainable = False\n",
        "        # features input (real sample)\n",
        "        real_features = Input(shape=INPUT_SHAPE)\n",
        "        # Noise input\n",
        "        z_disc = Input(shape=(LATENT_DIM,))\n",
        "        # Generate features based of noise (fake sample) and add label to the input\n",
        "        label = Input(shape=(N_ATTRIBUTES,))\n",
        "        fake_features = self.generator([z_disc, label])\n",
        "        # Discriminator determines validity of the real and fake images\n",
        "        fake = self.critic([fake_features, label])\n",
        "        valid = self.critic([real_features, label])\n",
        "        # Construct weighted average between real and fake images\n",
        "        interpolated_features = RandomWeightedAverage()(inputs=[real_features, fake_features, n_batch])\n",
        "        # Determine validity of weighted sample\n",
        "        validity_interpolated = self.critic([interpolated_features, label])\n",
        "        # Use Python partial to provide loss function with additional 'averaged_samples' argument\n",
        "        partial_gp_loss = partial(gradient_penalty_loss,\n",
        "                                  averaged_samples=interpolated_features)\n",
        "        partial_gp_loss.__name__ = 'gradient_penalty'  # Keras requires function names\n",
        "\n",
        "        c_model = Model(inputs=[real_features, label, z_disc], outputs=[valid, fake, validity_interpolated])\n",
        "        opt = Adam(lr=LEARNING_RATE, beta_1=BETA)\n",
        "\n",
        "        # c_model.compile(loss=['binary_crossentropy', 'binary_crossentropy', partial_gp_loss],\n",
        "        c_model.compile(loss=[wasserstein_loss, wasserstein_loss, partial_gp_loss],\n",
        "                        optimizer=opt, loss_weights=[1, 1, GRADIENT_PENALTY_WEIGHT],\n",
        "                        experimental_run_tf_function=False)\n",
        "        return c_model\n",
        "\n",
        "    def __compile_generator(self):\n",
        "        # For the generator we freeze the critic's layers + classification Layers\n",
        "        self.critic.trainable = False\n",
        "        self.classifier.trainable = False\n",
        "        self.generator.trainable = True\n",
        "\n",
        "        # Sampled noise for input to generator\n",
        "        z_gen = Input(shape=(LATENT_DIM,))\n",
        "        # add label to the input\n",
        "        label = Input(shape=(N_ATTRIBUTES,))\n",
        "        # Generate images based of noise\n",
        "        features = self.generator([z_gen, label])\n",
        "        # Discriminator determines validity\n",
        "        valid = self.critic([features, label])\n",
        "        # Discriminator determines class\n",
        "        classx = self.classifier(features)\n",
        "\n",
        "        g_model = Model([z_gen, label], [valid, classx])\n",
        "        opt = Adam(lr=LEARNING_RATE, beta_1=BETA)\n",
        "        g_model.compile(loss=[wasserstein_loss, SparseCategoricalCrossentropy(from_logits=True)],\n",
        "                        optimizer=opt, loss_weights=[1, CLS_LOSS_WEIGHT],\n",
        "                        experimental_run_tf_function=False)\n",
        "        return g_model\n",
        "\n",
        "    def train(self, trainX, trainA, trainY, n_batch=N_BATCH_GAN, n_epochs=N_EPOCHS_GAN, load=LOAD,\n",
        "              n_critic=N_CRITIC, latent_dim=LATENT_DIM, save_every=SAVE_MODEL_EVERY, save_path=MODEL_SAVE_PATH):\n",
        "        if load:\n",
        "            self.generator.load_weights(save_path)\n",
        "            return\n",
        "        # Adversarial ground truths\n",
        "        valid = -np.ones((n_batch, 1))\n",
        "        fake = np.ones((n_batch, 1))\n",
        "        dummy = np.zeros((n_batch, 1))  # Dummy gt for gradient penalty\n",
        "        for epoch in range(n_epochs):\n",
        "            for i in range(0, trainX.shape[0], n_batch):\n",
        "                for _ in range(n_critic):\n",
        "                    # Select a random batch of images\n",
        "                    idx = np.random.permutation(trainX.shape[0])[0:n_batch]\n",
        "                    features, labels, attr = trainX[idx], trainY[idx], trainA[idx]\n",
        "                    # Sample generator input\n",
        "                    noise = np.random.normal(0, 1, (n_batch, latent_dim))\n",
        "                    # Train the critic\n",
        "                    d_loss = self.compiled_critic.train_on_batch([features, attr, noise], [valid, fake, dummy])\n",
        "                noise = np.random.normal(0, 1, (n_batch, latent_dim))\n",
        "                g_loss = self.compiled_generator.train_on_batch([noise, attr], [valid, labels])\n",
        "            wass = -np.mean(d_loss[1] + d_loss[2])\n",
        "            print(\"%d [D loss: %f] [G loss: %f] [wass: %f] [real: %f] [fake: %f]\" % (\n",
        "            epoch, np.mean(d_loss[0]), np.mean(g_loss[0]), wass, np.mean(d_loss[1]), np.mean(d_loss[2])))\n",
        "            if epoch % save_every == 0:\n",
        "                #self.compiled_generator.save_weights(save_path)\n",
        "                self.generator.save_weights(save_path)\n",
        "\n",
        "    def generate_synth_dataset(self, signatures, labels, n_samples=N_SAMPLES_GAN, latent_dim=LATENT_DIM):\n",
        "        z_input = np.random.normal(0, 1, (n_samples, latent_dim))\n",
        "        fakeY = np.random.randint(0, len(labels), n_samples)\n",
        "        fakeY = labels[fakeY]\n",
        "        fakeA = signatures[fakeY]\n",
        "        fakeX = self.generator.predict([z_input, fakeA])\n",
        "        return fakeX, fakeA, fakeY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4r5GDXyIs5G"
      },
      "source": [
        "### Generative method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjpb56uEIwHu"
      },
      "source": [
        "# create and train classifier for GAN\n",
        "precls = Classifier()\n",
        "precls.train(train_seenX, train_seenY)\n",
        "precls.eval(test_unseenX, test_unseenY)\n",
        "precls.eval(test_seenX, test_seenY)\n",
        "\n",
        "# create and train GAN, create synthetic dataset\n",
        "gan = Gan(precls.get_model())\n",
        "gan.train(train_seenX, train_seenA, train_seenY)\n",
        "fake_trainX, fake_trainA, fake_trainY = gan.generate_synth_dataset(signatures, labels['test_unseen'])\n",
        "if GZSL:\n",
        "    fake_trainX = np.concatenate((fake_trainX, train_seenX))\n",
        "    fake_trainA = np.concatenate((fake_trainA, train_seenA))\n",
        "    fake_trainY = np.concatenate((fake_trainY, train_seenY.squeeze()))\n",
        "\n",
        "# create, train, and test final classifier\n",
        "# acc_seen=0.5014, acc_unseen=0.3985, h=0.4441, epoch=10\n",
        "postcls = Classifier()\n",
        "best_acc_seen, best_acc_unseen, best_H, best_epoch = 0, 0, 0, 0\n",
        "for epoch in range(N_EPOCHS_CLS):\n",
        "    postcls.train_epoch(fake_trainX, fake_trainY)\n",
        "    acc_seen = postcls.get_per_class_accuracy(test_seenX, test_seenY, labels['test_seen'])\n",
        "    acc_unseen = postcls.get_per_class_accuracy(test_unseenX, test_unseenY, labels['test_unseen'])\n",
        "    H = 2 * acc_seen * acc_unseen / (acc_seen + acc_unseen)\n",
        "    if H > best_H:\n",
        "        best_acc_seen, best_acc_unseen, best_H, best_epoch = acc_seen, acc_unseen, H, epoch\n",
        "print('acc_seen=%.4f, acc_unseen=%.4f, h=%.4f, epoch=%d' % (best_acc_seen, best_acc_unseen, best_H, best_epoch))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}