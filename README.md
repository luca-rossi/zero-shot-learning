# Zero Shot Learning

This repository contains:
- The material of a lecture on Zero-Shot Learning I gave in a Deep Learning university course
- A basic implementation (in the notebook and in the `trainer/` folder) of the paper: [Feature Generating Networks for Zero-Shot Learning](https://arxiv.org/abs/1712.00981)
- A partial implementation (in the `split_generator.py` file) of the split generator for the current paper I'm working on, where I analyze biases in the splits chosen by state-of-the-art method

The code in the `trainer/` folder has been adapted from another [repository](https://github.com/Abhipanda4/Feature-Generating-Networks), converted to a Keras implementation, fixed, refactored, and made easier to understand.

Before running the code, dowload the datasets from [here](https://datasets.d2.mpi-inf.mpg.de/xian/xlsa17.zip) and save them into the `data/` folder. Each subfolder in `data/` should have the same name as the dataset, and include the files `res101.mat` (with the features) and `att_splits.mat` (with the attributes).

## Basic notation

Let $k \in \mathbb{N}$ be the number of attributes, $d \in \mathbb{N}$ the number of features (also referred to as images or samples), $n \in \mathbb{N}$ the number of classes, with $n_S$ as the number of seen classes ($n_S < n$) and $n_U = n - n_S$ the number of unseen classes.

Let $A = \mathbb{R}^k$ be the semantic space (also referred to as the attribute space), $X = \mathbb{R}^d$ the feature space, and $C = {C_1 ... C_n}$ the set of all classes (seen and unseen). Let $C^S \subset C$ be the set of seen classes and $C^U = C \setminus C^S$ the set of unseen classes. Similarly, $X^S \subset X$ will refer to the samples belonging to seen classes and $X^U = X \setminus X^S$ to the samples belonging to unseen classes.

We define the signature of a class $a: C \to A$ as the function that maps each class to the attribute vector in the semantic space that uniquely identifies it. We define an attribute $a_i: C \to {0, 1}$ as a function indicating whether such attribute is present (with value 1) or not (with value 0) in that class or, alternatively, the $i^{th}$ element of the attribute vector returned by $a$. This is the definition of a binary attribute; we could define a continuous attribute with values included in $[0, 1]$ indicating its frequency in a class, but we will only consider binary attributes for simplicity.

## Description of the generated splits

This is a basic description of the splits generated by `split_generator.py`. More information about this work in an upcoming paper.

### Greedy Class Split (GCS).

The Greedy Class Split (GCS) tries to avoid the \"*horse with stripes* without *stripes* images\" scenario by keeping as much semantic information as possible among the seen classes.

In the binary definition of the semantic space, the value 1 indicates the presence of an attribute in an image, while the value 0 indicates its absence. This means that ones are more useful than zeros, so we maximize the former in the seen classes split.

In other words, for each class, we simply sum the values of its signature vector and we sort the classes by these sums in descending order. Consequently, we select the first $n_S$ classes as seen classes, and the other $n_U$ as unseen classes.

### Clustered Class Split (CCS).

Before introducing the Clustered Class Split (CCS), we need to define the Class Semantic Distance matrix $S^C = (s^C_{i,j}) \in \mathbb{R}^{n \times n}$ where $i, j \leq n$.

Then we define each element $s_{i,j} = l_2(c_i, c_j)$ as the euclidean distance between class $c_i$ and class $c_j$, where $l_2: C \times C \to \mathbb{R}$ is the distance between the two class signatures (attribute vectors).

With the CCS, we want to define the set of seen classes and the set of unseen classes as two clusters, meaning that we want to minimize the intra-cluster distances and maximize the inter-cluster distances.

Similar to the GCS, the clusters are defined by sorting the classes by the sum of their row (or column) values in descending order. The first $n_S$ classes are those with the lowest distances overall, meaning that they form a cluster in the semantic space. Those classes will be the seen classes. The other $n_U$ are far from this cluster in the semantic space, so they will form another cluster (although it is not a proper cluster since those classes are probably far away from each other as well), and they will be the unseen classes.

### Minimal Attribute Split (MAS).

Before introducing the Minimal Attribute Split (MAS), we define the Attribute Correlation matrix $S^A = (s^A_{i,j}) \in \mathbb{R}^{k \times k}$ where $i, j \leq k$.

Then we define each element $s_{i,j} = |O^C_{i,j}| / |O^C_{i}|$ as the ratio of co-occurrences of attributes $a_i$ and $a_j$ in all the classes $c \in C$. We define $O^C_{i} = \{c \in C: a_i(c) = 1\}$ as the set of classes with the attribute $i$. Similarly, we define $O^C_{i,j} = \{c \in C: a_i(c) = 1, a_j(c) = 1\}$ as the set of classes with both attributes $i$ and $j$.

With the MAS, we want to remove unnecessary attributes, i.e., we want to avoid highly correlated attributes.